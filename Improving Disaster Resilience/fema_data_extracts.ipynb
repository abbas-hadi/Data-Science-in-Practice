{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2400d85",
   "metadata": {},
   "source": [
    "# Project: Improving Disaster Resilience\n",
    "\n",
    "## FEMA data extracts\n",
    "\n",
    "### Extract and prepare FEMA provided data\n",
    "\n",
    "\n",
    "Andrew Sommers\n",
    "\n",
    "### Purpose\n",
    "\n",
    "- Extract FEMA provided datasets using the API\n",
    "- Extract any FEMA dataset provided via an API by providing the dataset name and version below\n",
    "- The FEMA API limits the nubmer of records that can be extracted per call to an API \n",
    "- This notebook extracts 1000 records per call until all records are extracted for the FEMA dataset that you designated\n",
    "- This notebook determines the length of the dataset and makes multiple calls to extract all records.\n",
    "- The extracted data is combined stored to a single csv file.\n",
    "\n",
    "\n",
    "##### Note:  the FEMA API's can be unstable.  For larger data files that require several extracts to obtain all of the records, be prepared for the extract to fail and throw an error message.  If the extract fails prior to all records being extracted, simply run the extract cell until you have a successful completion and output. \n",
    "\n",
    "\n",
    "\n",
    "#### History üóìÔ∏è\n",
    "\n",
    "Date | Person | Details\n",
    "---- | ------ | -------\n",
    "05/20/2023| Andrew Sommers | Create initial notebook\n",
    "11/08/2023| Andrew Sommers | Updated the documentation for the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8330ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and set runtime parameters\n",
    "#\n",
    "import pandas as pd # tabluar data\n",
    "from pandas import json_normalize #normalize json files to pandas dataframe\n",
    "#from functools import reduce\n",
    "#import requests # supports http requests to an api\n",
    "import numpy as np\n",
    "import math\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None # show all columns in display\n",
    "pd.options.display.max_rows = None # show all columns in display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cd52030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the notebook's working directory for storing the output file - change this for your local environment\n",
    "#\n",
    "# set the working directory where your'Data' folder is located;  this will be the folder for your output file.  \n",
    "# this notebook assumes data files are located in a 'Data' folder in the following path:\n",
    "os.chdir('C:\\\\Users\\\\andre\\\\OneDrive\\\\Documents\\\\IndianaUniversity\\\\D592\\\\Project_Disaster_Resilience\\\\Data\\\\FEMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbbc66aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters for FEMA API Extracts - designate the FEMA dataset to be extracted\n",
    "# each url requires a json_header and an output file to be designated\n",
    "#\n",
    "# By providing the fema file and file version, the API url can be built.\n",
    "# for the following example, the APU URL is https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries\n",
    "fema_file, fema_version = 'DisasterDeclarationsSummaries', 'v2' # Disasters Declaration Summary version 2\n",
    "base_url = 'https://www.fema.gov/api/open/'+fema_version+'/'+fema_file+'?' \n",
    "#\n",
    "top = 1000 # number of records to extract for each loop in the API extraction process\n",
    "skip = 0 # number of records to skip when extracting a records for a file thorugh the API\n",
    "base_url # this line prints the API extract address, so you can verify that you have the correct API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396bfac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of loops to extract all records from the FEMA dataset =  65\n"
     ]
    }
   ],
   "source": [
    "# Return 1st record with your criteria to get total record count. \n",
    "# This call requests the top '1' records to get the metadata record for the file\n",
    "# The metadata records provides the inlinecount to get record count.\n",
    "url = base_url + '$inlinecount=allpages&$select=id&$top=1'\n",
    "webUrl = urllib.request.urlopen(url)\n",
    "result = webUrl.read()\n",
    "jsonData = json.loads(result.decode())\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recordCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recordCount / top) # calculate the number of loops to extract all the data\n",
    "print(\"number of loops to extract all records from the FEMA dataset = \", loopNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bad1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1  complete\n",
      "iteration  2  complete\n",
      "iteration  3  complete\n",
      "iteration  4  complete\n",
      "iteration  5  complete\n",
      "iteration  6  complete\n",
      "iteration  7  complete\n",
      "iteration  8  complete\n",
      "iteration  9  complete\n",
      "iteration  10  complete\n",
      "iteration  11  complete\n",
      "iteration  12  complete\n",
      "iteration  13  complete\n",
      "iteration  14  complete\n",
      "iteration  15  complete\n",
      "iteration  16  complete\n",
      "iteration  17  complete\n",
      "iteration  18  complete\n",
      "iteration  19  complete\n",
      "iteration  20  complete\n",
      "iteration  21  complete\n",
      "iteration  22  complete\n",
      "iteration  23  complete\n",
      "iteration  24  complete\n",
      "iteration  25  complete\n",
      "iteration  26  complete\n",
      "iteration  27  complete\n",
      "iteration  28  complete\n",
      "iteration  29  complete\n",
      "iteration  30  complete\n",
      "iteration  31  complete\n",
      "iteration  32  complete\n",
      "iteration  33  complete\n",
      "iteration  34  complete\n",
      "iteration  35  complete\n",
      "iteration  36  complete\n",
      "iteration  37  complete\n",
      "iteration  38  complete\n",
      "iteration  39  complete\n",
      "iteration  40  complete\n",
      "iteration  41  complete\n",
      "iteration  42  complete\n",
      "iteration  43  complete\n",
      "iteration  44  complete\n",
      "iteration  45  complete\n",
      "iteration  46  complete\n",
      "iteration  47  complete\n",
      "iteration  48  complete\n",
      "iteration  49  complete\n",
      "iteration  50  complete\n",
      "iteration  51  complete\n",
      "iteration  52  complete\n",
      "iteration  53  complete\n",
      "iteration  54  complete\n",
      "iteration  55  complete\n",
      "iteration  56  complete\n",
      "iteration  57  complete\n",
      "iteration  58  complete\n",
      "iteration  59  complete\n",
      "iteration  60  complete\n",
      "iteration  61  complete\n",
      "iteration  62  complete\n",
      "iteration  63  complete\n",
      "iteration  64  complete\n",
      "iteration  65  complete\n",
      "API extracts and output file are complete\n"
     ]
    }
   ],
   "source": [
    "# Note:  the FEMA API's can be unstable.\n",
    "# For larger data files that require several extracts to obtain all of the records,\n",
    "# be prepared for the extract to fail and throw an error message. \n",
    "# the error from the API will look like: IncompleteRead: IncompleteRead(812148 bytes read)\n",
    "# If the extract fails prior to all records being extracted,\n",
    "# simply run this cell until you have a successful completion and output. \n",
    "# you might also need to shutdown other applications on your computer to free memory.\n",
    "\n",
    "\n",
    "# based on the number of extract loops to extract all records\n",
    "# run a series of loops to extract the records\n",
    "# combine the extracts\n",
    "# output the combine data to a csv file\n",
    "#\n",
    "i = 0\n",
    "while i < loopNum:\n",
    "    webUrl = urllib.request.urlopen(base_url + \"&$metadata=off&$skip=\" + str(skip) + \"&$top=\" + str(top))\n",
    "    response = webUrl.read()\n",
    "    jsonData = json.loads(response.decode())\n",
    "    if i == 0:\n",
    "        df1 = json_normalize(jsonData[fema_file])\n",
    "    else:\n",
    "        df2 = json_normalize(jsonData[fema_file])\n",
    "        df1 = df1.append(df2)\n",
    "    i += 1\n",
    "    skip = i * top\n",
    "    print('iteration ', i, ' complete')\n",
    "    \n",
    "df1.to_csv(fema_file+'.csv', header=True, index=False)  #output the combined extracted \n",
    "print('API extracts and output file are complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787be70",
   "metadata": {},
   "source": [
    "## End of FEMA Data Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9943",
   "metadata": {},
   "source": [
    "### appendix - FEMA datasets and description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd192ff",
   "metadata": {},
   "source": [
    "#### DisasterDeclarationsSummaries v2\n",
    "\n",
    "FEMA Disaster Declarations Summary is a summarized dataset describing all federally declared disasters. This dataset lists all official FEMA Disaster Declarations, beginning with the first disaster declaration in 1953 and features all three disaster declaration types: major disaster, emergency, and fire management assistance. The dataset includes declared recovery programs and geographic areas (county not available before 1964; Fire Management records are considered partial due to historical nature of the dataset)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
